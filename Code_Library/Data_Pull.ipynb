{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10af196b-3d82-497b-a46f-8e801dcc9b10",
   "metadata": {},
   "source": [
    "# Data Pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bab6e3-3178-4530-a62f-e26fb157b1b0",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dda57bc1-cdbe-41bc-bfdb-d0648a11623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4291330-f456-40ff-b56e-109a6aa6c0c7",
   "metadata": {},
   "source": [
    "## Perform Web Scrape from Craigslist in Los Angeles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e9028-28e2-49d1-8d56-95061842cfb4",
   "metadata": {},
   "source": [
    "**A Note on the Default Search**\n",
    "\n",
    "The default starting page will have certain characteristics already standardized for the purposes of this project. I have narrowed the focus to only 1 bedroom, 1 bathroom apartments. There is also a specific map area being used. The map view is an area centered around Santa Monica, and encompasses the West Los Angeles region north of Manahattan Beach and south of Pacific Palisades. This area contains a high density of apartments, giving us a steady supply of data to pull.\n",
    "\n",
    "This strategy pulls many results and isolates two big cost factors in the forthcoming regression equation. This allows us to better view the effects of engineered features, which we will perform later. \n",
    "\n",
    "Our goal here is to cater the data science insights to me (the author) first with an eye towards scaling to a potential use case by anyone. Thus, while we will engineer the data infrastructure with an eye towards scaling the data quantity and features, we want to narrow the insights to be useful to at least one person (myself) before we expand further. This lets us behave pragmatically within the time constraints of a 7-week project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9563c747-ec42-4b1a-81dc-d557a61f475e",
   "metadata": {},
   "source": [
    "#### First, we declare global variables that will be used in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2541a3-38be-4430-a239-ee79738e7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global Variables ##\n",
    "\n",
    "craigslist_base_url = 'https://losangeles.craigslist.org/search/santa-monica-ca/apa?lat=34.0315&lon=-118.461&max_bathrooms=1&max_bedrooms=1&min_bathrooms=1&min_bedrooms=1&postal=90095&search_distance=3.6#search=1'\n",
    "craigslist_search_first_page_url = 'https://losangeles.craigslist.org/search/santa-monica-ca/apa?lat=34.0315&lon=-118.461&max_bathrooms=1&max_bedrooms=1&min_bathrooms=1&min_bedrooms=1&postal=90095&search_distance=3.6#search=1~list~0~0'\n",
    "chrome_driver_path = '../Other_Material/chromedriver-mac-arm64/chromedriver'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c631c0b-caf7-4d35-a213-2d8d2de7259b",
   "metadata": {},
   "source": [
    "#### We will be using BeautifulSoup to access URLs in this notebook, so we write a function to perform this operation now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6580d9f-841b-42e1-9e74-12767cae04f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the access_beautiful_soup function\n",
    "def access_beautiful_soup(url):\n",
    "    # Call a get instance with the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Sleep in order to not overwhelm servers\n",
    "    time.sleep(5 + 10 * random.random())\n",
    "\n",
    "    # Find all the listings links on the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ec4ce-5b79-4bbc-96a7-1183c0c9321f",
   "metadata": {},
   "source": [
    "#### Before we access the URLs of the listings, we need to find out how many there are in the search query. \n",
    "\n",
    "Since this information is not accessible via BeautifulSoup based on the way the Craigslist HTML structure is set up, we need to use Selenium. \n",
    "\n",
    "We now proceed with implementing the Selenium code to get the total listings amount. We write a function called \"get_postings_count\" that takes in the two arguments \"website\" and \"path\" and returns the post count. We need to use JavaScript here, because the post count is loaded dynamically into the webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54513ff2-acb8-4480-9222-a086b40c1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the number of posts at a given time\n",
    "def get_postings_count(website, path):\n",
    "    \n",
    "    # prevent a window from opening in Selenium\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    \n",
    "    # set up the Chrome driver path for Selenium usage\n",
    "    service = Service(path)\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    # Call a \"get\" instance of the initial Craigslist page to initialize Selenium\n",
    "    driver.get(website)\n",
    "\n",
    "    # Use a waiting period to make sure all the elements load for Selenium to inspect\n",
    "    wait = WebDriverWait(driver, 10)  # Wait for up to 10 seconds\n",
    "\n",
    "    try:\n",
    "        # Wait for the specific element to be present before executing the script\n",
    "        postings_count_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.cl-count-save-bar > div')))\n",
    "\n",
    "        # Use JavaScript to set up a script to return the postings count\n",
    "        postings_count_script = \"\"\"\n",
    "            var postingsDiv = document.querySelector('.cl-count-save-bar > div');\n",
    "            return postingsDiv ? postingsDiv.textContent : 'Postings count not found';\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute the script to get the post count and return it\n",
    "        postings_count = driver.execute_script(postings_count_script)\n",
    "        return postings_count\n",
    "    finally:\n",
    "        # Exits Selenium\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51254b18-808f-42ac-9888-462009d47dd9",
   "metadata": {},
   "source": [
    "#### Let's call the function now to get the postings count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1e06641-4dd2-45d3-9dd9-4984f920f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the get_postings_count function\n",
    "postings_count = get_postings_count(craigslist_search_first_page_url, chrome_driver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24c6798-5d22-40b0-81b9-42868e090c84",
   "metadata": {},
   "source": [
    "#### Finally, we print the amount of posts to see how many we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ff4a5a-74a6-4381-8c8c-6e5d26eb9d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,353 postings\n"
     ]
    }
   ],
   "source": [
    "# Check to see how many postings there are\n",
    "print(postings_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f38f78-84c5-4c5d-8b51-d912fbc05c54",
   "metadata": {},
   "source": [
    "#### We can now use the post count to get the number of pages to loop through. \n",
    "\n",
    "There are 120 posts per page, so we want to extract the post count and divide it by 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d2173c4-61f7-460b-974b-e4ca9c3400bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the number of pages for us to loop through\n",
    "def calculate_pages_from_postings(postings_count_str):\n",
    "    \n",
    "    # Remove commas and extract the numerical part of the string\n",
    "    num_postings = int(postings_count_str.replace(\" postings\", \"\").replace(\",\", \"\"))\n",
    "    \n",
    "    # 120 posts per page\n",
    "    postings_per_page = 120\n",
    "    \n",
    "    # Calculate the number of pages needed to display all postings, accounting for remainder\n",
    "    num_pages = -(-num_postings // postings_per_page)  \n",
    "    \n",
    "    return num_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a101ea-d031-43e6-a033-a51ced126038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 20\n"
     ]
    }
   ],
   "source": [
    "# Call the calculate_pages_from_postings function\n",
    "number_of_pages = calculate_pages_from_postings(postings_count)\n",
    "print(f\"Number of pages: {number_of_pages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f114a6e-e32e-4ff5-a502-709fa8145890",
   "metadata": {},
   "source": [
    "#### Next, we write a function to extract the links from each of the pages. We do this by: \n",
    "\n",
    "1. initializing a list\n",
    "2. looping through the number of pages\n",
    "3. finding all the 'li' tags within page that use the class 'cl-static-search-result'\n",
    "4. finding the 'a' tags within the 'li' tags, which contain the link to the individual listing\n",
    "5. appending these links to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9000ca2b-8a3d-403e-a996-fc4433872e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the extract_listings_links function\n",
    "def extract_listings_links(number_of_pages):\n",
    "    \n",
    "    # Initialize a list to store the links\n",
    "    listings_links = []\n",
    "\n",
    "    # Iterate through the number of pages\n",
    "    for i in range(number_of_pages):  \n",
    "\n",
    "        # Use the page number to get the webpages containing the listings\n",
    "        page_number = i\n",
    "        page_url = f'{craigslist_base_url}~list~{page_number}~0'\n",
    "\n",
    "        # Access the URL using beautiful soup function\n",
    "        soup = access_beautiful_soup(page_url)\n",
    "\n",
    "        # Look for all 'li' tags with the class 'cl-static-search-result'\n",
    "        listings = soup.find_all('li', class_='cl-static-search-result')\n",
    "\n",
    "        # Loop through all the listings and append links to the list\n",
    "        for listing in listings:\n",
    "            a_tag = listing.find('a', href=True)\n",
    "            if a_tag:\n",
    "                listings_links.append(a_tag['href'])\n",
    "\n",
    "    return listings_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4818f2c7-cd85-4e0e-9d7b-840d91ae53b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the extract_listings_links function and store the returned list in the 'all_links' variable\n",
    "all_links = extract_listings_links(number_of_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261360ec-1f4b-4ac8-aaa5-fff7d7080612",
   "metadata": {},
   "source": [
    "Let's check out the \"all_links\" list to see the extraction was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "229b01ff-8ef6-4130-9b8a-4051bc724375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n"
     ]
    }
   ],
   "source": [
    "print(len(all_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1e1b3-0e53-4288-859f-91ae173706d4",
   "metadata": {},
   "source": [
    "We see that there is content within the all_links list. Next, let's get a sample of three of the links to ensure we pulled what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90e8f55b-2f07-4a91-94b4-0208faebf590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link #1:  https://losangeles.craigslist.org/wst/apa/d/venice-bedroom-in-marina-del-rey-quartz/7726576879.html\n",
      "Link #2:  https://losangeles.craigslist.org/wst/apa/d/los-angeles-bedroom-ba-in-west-la/7726576204.html\n",
      "Link #3:  https://losangeles.craigslist.org/wst/apa/d/los-angeles-westwood-bedroom-bath/7726575683.html\n"
     ]
    }
   ],
   "source": [
    "print(f'Link #1: ',all_links[0])\n",
    "print(f'Link #2: ',all_links[1])\n",
    "print(f'Link #3: ',all_links[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c8d8ac-9841-4513-8f78-c797537edc52",
   "metadata": {},
   "source": [
    "#### We now dive into working with the data within each listing link.\n",
    "\n",
    "First, we set up a dataframe containing basic column information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "59268f23-ad4e-4bca-9a5b-829e0d7514c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataFrame\n",
    "df_columns = [\"Title\", \"Price\", \"Bedrooms\", \"Square Feet\", \"Full Address\"]\n",
    "listings_df = pd.DataFrame(columns=df_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86efbe-7455-4714-8fe7-57986f70e753",
   "metadata": {},
   "source": [
    "#### Next, we begin the process of creating boolean values for different attributes.\n",
    "\n",
    "Each listing contains different attributes. While there is a lot of overlap, we need to see all of the options. To do this, we initialize a dictionary called \"global_attribute_counts,\" then add unique values and count them. Ultimately, we want to create columns with these values and use boolean values \"1\" or \"0\" meaning \"present\" or \"not present.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6699b64e-8f6e-4046-9e86-9cfbbfe5087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = ['https://losangeles.craigslist.org/wst/apa/d/venice-bedroom-in-marina-del-rey-quartz/7726576879.html',\n",
    "             'https://losangeles.craigslist.org/wst/apa/d/los-angeles-bedroom-ba-in-west-la/7726576204.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7918256f-c3e0-458e-a46d-83c705ad128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_and_soups = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7882c75d-d3fa-4ebc-acb8-f8274c82a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_links_and_soups(list_of_links):\n",
    "    for link in list_of_links:\n",
    "        the_soup = access_beautiful_soup(link)\n",
    "        links_and_soups[link] = the_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5012d4ee-f5ac-498d-8e5a-36e5605ecc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_links_and_soups(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "30acfc6b-ebb3-4630-8574-b5f8c16ecaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dictionary for counting attributes across all listings\n",
    "global_attribute_counts = {}\n",
    "\n",
    "attributes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a28e68dc-ed92-403c-b11a-0875f5801090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_basic_information(the_soup):\n",
    "    title = the_soup.find(\"span\", id=\"titletextonly\").text.strip()\n",
    "    price = the_soup.find(\"span\", class_=\"price\").text.strip()\n",
    "    bedroom_info = the_soup.find(\"span\", class_=\"housing\").text.split(\"/\")[1].split(\"-\")[0].strip()\n",
    "    square_feet = the_soup.find(\"span\", class_=\"housing\").text.split(\"-\")[1].split(\"ft\")[0].strip()\n",
    "    \n",
    "    full_address_element = the_soup.find(\"h2\", class_=\"street-address\")\n",
    "    if full_address_element:\n",
    "        full_address = full_address_element.text.strip()\n",
    "    else:\n",
    "        full_address = \"None listed\"\n",
    "\n",
    "    return title, price, bedroom_info, square_feet, full_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "213fbb51-8b4a-46c2-a74f-3cf7ea2fba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the count_attributes_function to view all the attributes used in apartment listings\n",
    "def process_attributes(the_soup):\n",
    "    attribute_search = the_soup.find_all('div', class_='attr')\n",
    "    attributes = []\n",
    "    for listing in attribute_search:\n",
    "        value_span = listing.find('span', class_='valu')\n",
    "        if value_span:\n",
    "            attribute = value_span.text.strip()\n",
    "            attributes.append(attribute)\n",
    "            # Update global attribute counts\n",
    "            global_attribute_counts[attribute] = global_attribute_counts.get(attribute, 0) + 1 \n",
    "            \n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fe7cc134-4327-42ee-bb72-3dd8a1a5a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the count_attributes_function to view all the attributes used in apartment listings\n",
    "def create_dataframe(links_and_soups, listings_df):\n",
    "    localized_df = listings_df.copy()\n",
    "    \n",
    "    for link, soup in links_and_soups.items():\n",
    "\n",
    "        title, price, bedroom_info, square_feet, full_address = collect_basic_information(soup)\n",
    "        attributes = process_attributes(soup)\n",
    "        \n",
    "        # Append the information as a new row in the DataFrame\n",
    "        new_row = pd.Series([title, price, bedroom_info, square_feet, full_address], index=df_columns)\n",
    "        localized_df = pd.concat([localized_df, new_row.to_frame().T], ignore_index=True)\n",
    "    \n",
    "    return localized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0b91413e-1140-444e-87a4-da6a88689681",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = create_dataframe(links_and_soups, listings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c9ebe38d-9782-4045-a18e-71b0249d6788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'monthly': 2,\n",
       " 'air conditioning': 1,\n",
       " 'cats are OK - purrr': 2,\n",
       " 'apartment': 2,\n",
       " 'laundry on site': 2,\n",
       " 'off-street parking': 2}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_attribute_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "288767b0-bac8-43af-8956-655369433835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Bedrooms</th>\n",
       "      <th>Square Feet</th>\n",
       "      <th>Full Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 Bedroom in Marina Del Rey -Quartz Counters -...</td>\n",
       "      <td>$3,295</td>\n",
       "      <td>1br</td>\n",
       "      <td>750</td>\n",
       "      <td>415 Washington Boulevard, Venice, CA 90292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 Bedroom 1 BA in West L.A. | Hardwood Style F...</td>\n",
       "      <td>$2,250</td>\n",
       "      <td>1br</td>\n",
       "      <td>700</td>\n",
       "      <td>None listed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title   Price Bedrooms  \\\n",
       "0  1 Bedroom in Marina Del Rey -Quartz Counters -...  $3,295      1br   \n",
       "1  1 Bedroom 1 BA in West L.A. | Hardwood Style F...  $2,250      1br   \n",
       "\n",
       "  Square Feet                                Full Address  \n",
       "0         750  415 Washington Boulevard, Venice, CA 90292  \n",
       "1         700                                 None listed  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
